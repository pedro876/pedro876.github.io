<h1>Visibility Bitmask Ambient Occlusion</h1>
<h2>
    October 23rd, 2025 <br />
    Unreleased Project
</h2>

<h3>Motivation</h3>

<p>
    A few months ago I wrote an article about baking Ambient Occlusion (AO) as vertex attributes. It was a good approach because we were going for a retro look in that project in particular.
    However, I'm currently working on a completely different game that requires more realistic lighting.
</p>

<p>
    Baking lighting as lightmaps was our solution for some time, but it makes iteration slow, makes it impossible for artists to obtain immediate feedback of their 
    changes in engine, and takes a lot of time to bake. Therefore, we have scratched that approach entirely and decided to go for real-time lighting. 
</p>

<p>My last article about Soft Shadows explains how we solved direct lighting, but indirect lighting is a different beast entirely. We have been using a rudimentary version of 
    SSAO for some time combined with Sparse Voxel GI, but we needed a better AO solution. </p>

<p>
    After doing some research for a while, I stumbled upon a paper from 2023 titled <a target="_blank" href="https://arxiv.org/abs/2301.11376">Screen Space Indirect Lighting with Visibility Bitmask</a>.
    It builds on the foundations laid by <a target="_blank" href="https://www.activision.com/cdn/research/PracticalRealtimeStrategiesTRfinal.pdf">Ground-Truth Ambient Occlusion</a>, achieving results 
    that are closer to a raytraced solution and with more intuitive parameters in my opinion.
</p>

<p>
    Visibility Bitmask Ambient Occlusion (VBAO) is a great fit for our game as is, but using a custom render pipeline allows us to
    provide better inputs to it and obtain more appealing results, as I will show in this article.
</p>

<p style="color:red">Imagen de portada</p>

<h3>Implementation</h3>

<h4>History</h4>

<p>
    First of all, I'm not going to explain every single detail about how to implement VBAO, because you can already find that information
    online. Instead, I will give an overview of the technique, explain how we have integrated it in our render pipeline 
    and provide source code. If you are not familiar with Screen-Space Ambient Occlusion, I suggest you go in chronological order 
    and read these resources before you continue:
</p>

<ol>
    <li><a target="_blank" href="https://learnopengl.com/Advanced-Lighting/SSAO">SSAO</a>: First implemented for Crysis in 2007.</li>
    <li><a target="_blank" href="https://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf">HBAO</a>: Presented by Nvidia in 2008, popularized by <i>Battlefield 3</i></li>
    <li><a target="_blank" href="https://www.activision.com/cdn/research/PracticalRealtimeStrategiesTRfinal.pdf">GTAO</a>: Presented by Activision in 2016, used for the first time in <i>Call of Duty: WWII</i></li>
    <li><a target="_blank" href="https://github.com/GameTechDev/XeGTAO">XeGTAO</a>: Intel's implementation of GTAO.</li>
    <li><a target="_blank" href="https://arxiv.org/abs/2301.11376">VBAO</a>: Published in 2023 by Olivier Therrien, Yannick Levesque, and Guillaume Gilet.</li>
</ol>

<p>
    While there have been many other attempts at solving AO, I consider these as the most important ones in order to understand 
    how we've gone from the first SSAO implementation back in 2007 to the present.
</p>

<p>
    I should mention that RTAO is another popular technique to solve Ambient Occlusion, but it requires raytracing and is not a screen-space effect, so 
    I will cover it in a separate article some time in the future.
</p>

<h4>Overview</h4>

<p>
    VBAO is horizon-based, it doesn't trace rays or sample random points inside a hemisphere in screen-space as the traditional SSAO implementation. Instead, thehemisphere is split into slices,
    and each slice into two sides. For each side, a horizon angle is calculated by sampling the depth buffer along the slice direction of that side.
    Finally, based on the horizon angles, we can determine how much of the slice is occluded. The combined occlusion of each slice gives an approximation of the total hemisphere occlusion.
    This was first done in HBAO and improved later in GTAO.
</p>

<p style="color:red">Poner imagen explicativa del HBAO</p>

<p>
    In VBAO, the overall structure of the shader is the same as in GTAO, but in this case we won't calculate the maximum horizon angles for each side of every slice.
    Instead, slices are split into 32 sectors, and a single unsigned int is used as a bitmask to store which sectors of the slice are occluded. For each sample of the 
    depth buffer, two positions are determined:
</p>

<ul>
    <li>Front Position: The exact point where the surface starts in view space using the unmodified depth buffer value.</li>
    <li>Back Position: An estimated point where the surface ends in view space, using an offsetted depth buffer using a constant thickness value.</li>
</ul>

<p>
    Then, those positions are transformed into two sector indices and all sectors in between are flagged as occluded for that slice. Once we have finished 
    calculating the slice occlusion, its bits are counted and added to the overall hemisphere occlusion.
</p>

<p>This technique improves the results of GTAO because it can simulate the behavior of light passing behind objects, so thin objects are not over-occluded.</p>

<p style="color:red">Mostrar imagen comparativa de GTAO vs VBAO en los cables del techo de Lair.</p>



<h4>Multi-Layered Depth</h4>

<p>
    Usually a game renders to a color and depth buffers. Since we're using deferred rendering, there are also 
    a variety of textures (G-Buffer) used for different purposes such as PBR properties, normals, emission and more.
    
</p>

<p>
    When using screen-space effects such as SSAO or SSR, we'd typically just have information about the closest surfaces
    to the camera per-pixel because the depth buffer only holds a single value. However, our render pipeline has an option 
    to enable multi-layered depth that currently supports a secondary
    G-Buffer. Objects are assigned a layer target based on their rendering layer.
</p>

<p>
    For example, all characters are rendered
    to the second G-Buffer, which allows screen-space effects to see behind them. This feature has a considerable cost
    because more textures are allocated, lighting is performed twice, and more texture fetches are required. In practice, 
    using only two layers is fast enough that the benefits are worth it. Most games using screen space effects suffer from
    occlusion artifacts, generating strange silhouettes below objects in SSR, or halos around characters in SSAO.
    While those problems can be resolved completely using raytracing, our solution is more performant and solves the most
    obvious and annoying cases where screen-space effects usually fail.
</p>

<h4>Per-Material Thickness</h4>

<p>
    If you follow the original VBAO paper, you can find a comparison of different thickness values compared against 
    a raytraced reference. While it is commendable how good this algorithm looks with a constant thickness value, 
    it is logical that having more precise thickness data per-pixel would lead to a more accurate solution.
</p>

<p>
    To support that, we have introduced a thickness parameter that can be configured per material. This allows
    artists to modify the occlusion contribution of objects in an isolated way. The value is encoded into
    16 bits in the G-Buffer along with other physical properties. For example, foliage
    should use a very small thickness value to avoid over-occlusion, while big objects benefit from a larger thickness
    value. Now, the global thickness parameter becomes a multiplier of the per-material value.
</p>

<p style="color:red">Aquí estaría bien mostrar un personaje en una sala con foliage a su alrededor, y comparar el resultado de usar un thickness global vs multi-layer con thickness ajustado por material.</p>

<h4>Calculating Occlusion</h4>

<p>
    This VBAO implementation is very pipeline-specific, so bear that
    in mind when checking the code below. The execution varies depending on the keyword
    named _MULTI_LAYER_DEPTH. When disabled, the code is a straightforward VBAO implementation 
    with some tweaks that uses a single-value depth buffer and G-Buffer normals. However, 
    when the keyword is enabled the shader has access to two different depth values, their normals
    and their thickness.
</p>

<div class="code" data-lang="hlsl" data-url="Articles/VBAO/CalculateVBAO.hlsl"></div>

<h4>Denoising</h4>

<p>
    Since we can't sample every single position inside the hemisphere because it would make any GPU explode, 
    we need to calculate ambient occlusion stochastically. This means that the sampled positions vary spatially and 
    temporally as well. A noise function must be used that gives us two different random values per pixel. One value is used 
    to rotate the hemisphere slices, and the other to apply an offset to the samples along each slice.
</p>

<p>
    I tried using blue noise, and spatio-temporal blue noise (STBN). However, it didn't get along very well with 
    neighbourhood clipping in the TAA pass. So I scrachted that approach, I needed a noise patter with very low discrepancy 
    between pixels so that TAA would work well with it. In XeGTAO, they used a Hilbert Curve of level 6 (64x64 pattern repetion)
    to turn a 2D screen coordinate into a 1D index. Then, that index is used to sample a low discrepancy sequence, more precisely 
    <a target="_blank" href="https://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/">Martin Roberts R2 sequence</a>.
</p>

<p>
    Therefore, it is more convenient to spatially denoise AO before TAA is applied. To do so, we can 
    use a bilateral filtering that uses depth information to preserve edges. In our case, using a separable
    box bilateral filter provided good results:
</p>

<div class="code" data-lang="hlsl" data-url="Articles/VBAO/DenoiseVBAO.hlsl"></div>

<h3>Results</h3>

<h3>Conclusion</h3>

<p>
    I was a bit skeptical that using VBAO would make a great difference compared to SSAO, but 
    decided to give it a try anyway. I'm glad I did, because the game's lighting has massively 
    improved thanks to it. Indirect lighting is much more plausible now, and screen-space artifacts 
    are highly minimized.
</p>

<p style="color:red">Sería buena idea poner una comparativa de SSAO vs VBAO</p>

<p>
    I honestly don't think it gets much better than this when it comes to screen-space Ambient Occlusion, but 
    then again, I thought the same when I first saw GTAO. Maybe in a few years someone will come up with an even 
    better solution that does not require raytracing. But until then, I'm very satisfied with what we've got.
</p>

<h3>Going Forward</h3>

<p>
    There are some aspects of the implementation that could still be improved, these are some ideas that come to mind:
</p>

<ul>
    <li><b>Depth Mip Chain</b>: In XeGTAO, it is mentioned that a depth buffer mip chain is created to speed up the algorithm. The idea is to use 
    increasingly higher mips (less resolution) as the distance from the hemisphere origin grows. 
    One of the reasons most SSAO algorithms are so costly is because of cache trashing, so using lower resolution textures for most samples
    could in fact improve performance notoriously.
    </li>

    <li><b>Better Noise Pattern</b>: While spatial denoising combined with TAA smooths the occlusion result greatly.
    I have noticed that it doesn't perfectly converge over time, which leads to some noise flickering. I know a lot of it 
    comes down to the noise pattern used to calculate AO, so I will do some research about it in the future. Maybe 
    Interleaved Gradient Noise could lead to more stable outputs.</li>

    <li><b>Per-Vertex Thickness</b>: While per-material thickness is a huge improvement over a global thickness
    parameter, sometimes it's not enough to represent some complex models. An idea that comes to mind would 
    be preprocessing models to generate a vertex attribute that acts as an average thickness value per 
    vertex. That way we would have very precise information to better guide the VBAO algorithm, but 
    we'd probably need more than 16 bits in the G-Buffer for it!</li>

    <li><b>Raytraced Ambient Occlusion (RTAO)</b>: Implementing RTAO would not only be great to have as an option 
    for capable GPUs, it would also help us parameterize VBAO in a way that more closely matches a raytraced 
    reference in-engine.</li>
</ul>

<p>
    That said, I consider Ambient Occlusion to be one of the most entertaining effects to implement and learn about 
    as a graphics programmer. I think there is so much one can learn about rendering, mathematics and physics, 
    just from implementing this algorithm, that I can't believe it took me so long to delve into it.
</p>

<p>
    While I consider this a huge milestone, there is still so much to learn and so many ways to improve 
    our render pipeline. What should be next? Maybe SSGI would be great!
</p>